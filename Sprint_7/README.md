# Sprint 7: Curso de Spark

Nesta sétima sprint da trilha de aprendizado em Engenharia de Dados, concluí um curso completo e atualizado sobre **Apache Spark**, uma das ferramentas mais importantes e versáteis para processamento de dados na era do Big Data. Este curso aprofundou meus conhecimentos sobre processamento distribuído, integração com outras ferramentas e bibliotecas, e aplicação prática de conceitos avançados em projetos de dados.

---

## Introdução ao Curso de Spark

O **Apache Spark** é amplamente reconhecido como uma solução essencial para Cientistas e Engenheiros de Dados devido à sua capacidade de processar grandes volumes de dados com alta performance. Ele combina processamento distribuído, paralelismo e otimizações em memória, sendo ideal para análises complexas, modelagem de Machine Learning e processamento em tempo real. Neste curso prático, explorei desde conceitos básicos até técnicas avançadas, tornando-me apto a integrar o Spark ao meu conjunto de habilidades em ciência e engenharia de dados.

---

## Principais Tópicos Abordados

### Fundamentos do Spark
- **Instalação e Configuração**: Configuração do ambiente Spark, garantindo funcionalidade local e em cluster.
- **DataFrames no Spark**: Manipulação de dados utilizando transformações e ações para criar pipelines de processamento eficientes.

### Processamento e Consultas
- **SQL no Spark**: Execução de consultas utilizando sintaxe SQL, criação de views e execução de joins.
- **Persistência de Dados**: Armazenamento de dados em formatos otimizados como Parquet e ORC.

### Integrações e Importação de Dados
- **Conexões Externas**: Integração com fontes de dados como MongoDB, PostgreSQL e arquivos em formatos JSON e Parquet.
- **Integração com Python**: Uso do PySpark para processamento de dados e integração com bibliotecas como Pandas.

### Machine Learning e Streaming
- **Modelagem com Spark MLlib**: Criação de modelos de Machine Learning e previsão de dados.
- **Pipelines de Machine Learning**: Construção de fluxos de trabalho otimizados para análise preditiva.
- **Streaming Estruturado**: Processamento de dados em tempo real com o Spark Structured Streaming.

### Otimização e Escalabilidade
- **Otimização de Performance**: Técnicas como cache, persistência, particionamento e bucketing.
- **Construção de Clusters**: Configuração de clusters Spark para processar grandes volumes de dados.

### Ferramentas e Utilização Prática
- **Uso com Jupyter Notebooks**: Execução e visualização de dados diretamente no Jupyter.
- **Scripts e Aplicações**: Desenvolvimento de scripts para rodar aplicações diretamente na linha de comando.

---

## Evidências

[Evidências](./Evidencias/)



## Exercícios

[Exercícios](./Exercicios/)


## Certificados

[Certificados](./Certificados/)